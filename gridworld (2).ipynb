{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b849259",
   "metadata": {},
   "source": [
    "<font size=\"5\" color=\"#2874a6\">Reinforcement Learning with Q-learning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3312ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba1661",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30eb8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Environment class\n",
    "class Environment:\n",
    "    def __init__(self, shape, landmarks: list = None):\n",
    "        self.shape = shape\n",
    "        self.landmarks = landmarks\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Defines and resets the entire state space.\"\"\"\n",
    "        self.state = None\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Defines the next state and reward regarding to an action at current state.\"\"\"\n",
    "        reward, done = None, False\n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62879f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, get_args\n",
    "\n",
    "# Movement types\n",
    "_MOVETYPES = Literal[\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "# GridEnvironment class\n",
    "class GridEnvironment(Environment):\n",
    "    def __init__(self, shape=(4, 4), landmarks=[(0, 3), (1, 1)]):\n",
    "        super().__init__(shape, landmarks)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (3, 0)\n",
    "        return self.state\n",
    "    \n",
    "    def give_reward(self):\n",
    "        if self.state == self.landmarks[0]:\n",
    "            return 1\n",
    "        elif self.state == self.landmarks[1]:\n",
    "            return -1 # penalty\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return self.state == self.landmarks[0]\n",
    "    \n",
    "    def step(self, action: _MOVETYPES):\n",
    "        next_state = list(self.state)\n",
    "        match(action):\n",
    "            case \"up\":\n",
    "                next_state[0] = max(0, self.state[0] - 1)\n",
    "            case \"down\":\n",
    "                next_state[0] = min(self.shape[0] - 1, self.state[0] + 1)\n",
    "            case \"left\":\n",
    "                next_state[1] = max(0, self.state[1] - 1)\n",
    "            case \"right\":\n",
    "                next_state[1] = min(self.shape[1] - 1, self.state[1] + 1)\n",
    "        self.state = tuple(next_state)\n",
    "\n",
    "        reward = self.give_reward()\n",
    "        done = self.is_terminal()\n",
    "\n",
    "        return self.state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba263b15",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d59f0",
   "metadata": {},
   "source": [
    "The Q-learning algorithm uses the following formula to update the Q-value for a state-action-pair:\n",
    "\n",
    "$Q(s,a) = Q(s,a) + \\alpha \\cdot [R + \\gamma \\cdot \\underset{a} \\max\\ Q(s',a') - Q(s,a)]$\n",
    "\n",
    "with:\\\n",
    "$Q(s,a)$ : Q-value for state $s$ and action $a$\\\n",
    "$\\alpha$ : learning rate\\\n",
    "$R$ : immediate reward for taking action $a$ in state $s$\\\n",
    "$\\gamma$ : discount factor, representing the importance of future rewards\\\n",
    "$\\max Q(s',a')$ : maximum Q-value for the the next state $s'$, representing the best possible reward achievable from that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46aa45d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Base Agent class\n",
    "class Agent:\n",
    "    pass\n",
    "\n",
    "# QLearning Agent class\n",
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, environment: Environment, actions: list = [], alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.environment = environment\n",
    "        self.actions = list(actions)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        # Q-table\n",
    "        # A Q-value, denoted with Q(s,a), represents the expected cumulative reward for choosing action a in state s\n",
    "        self.q_table = np.zeros(self.environment.shape + (len(self.actions),))\n",
    "\n",
    "    def best_action(self, state):\n",
    "        return self.actions[np.argmax(self.q_table[state])]\n",
    "    \n",
    "    # The agent uses an epsilon-greedy strategy\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return self.actions[np.random.randint(len(self.actions))] # explore\n",
    "        else:\n",
    "            return self.best_action(state) # exploit\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        a = self.actions.index(action)\n",
    "        best_next_q = np.max(self.q_table[next_state])\n",
    "        self.q_table[state][a] += self.alpha * (reward + self.gamma * best_next_q - self.q_table[state][a])\n",
    "\n",
    "    def fit(self, episodes=1000, verbose=False):\n",
    "        for episode in range(episodes):\n",
    "            state = self.environment.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.environment.step(action)\n",
    "                self.update_q_value(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Episode: {episode}, Total reward: {total_reward}\")\n",
    "\n",
    "    def path(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.environment.reset()\n",
    "        else:\n",
    "            self.environment.state = state\n",
    "\n",
    "        states = [state]\n",
    "        actions = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.best_action(state)\n",
    "            state, reward, done = self.environment.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            if done:\n",
    "                break\n",
    "        return states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014e897",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc2e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnvironment()\n",
    "agent = QLearningAgent(environment=env, actions=get_args(_MOVETYPES))\n",
    "agent.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3982b8",
   "metadata": {},
   "source": [
    "## Use Agent for Optimal Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e537d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal path to goal: [(2, 1), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (0, 3)]\n",
      "Optimale moves from state to goal: ['left', 'up', 'up', 'right', 'right', 'right']\n"
     ]
    }
   ],
   "source": [
    "states, actions = agent.path(state=(2, 1))\n",
    "print(\"Optimal path to goal:\", states)\n",
    "print(\"Optimale moves from state to goal:\", actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
